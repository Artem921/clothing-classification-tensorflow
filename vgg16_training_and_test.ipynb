{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "path='...name/.csv'\n",
    "data=pd.read_csv(path, error_bad_lines=False)\n",
    "data.drop(data.index[6500:],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images=[]\n",
    "for i in data.id:\n",
    "    img='images/'+str(i)+'.jpg'\n",
    "    images.append(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "labencod=LabelEncoder()\n",
    "labencod.fit(data.articleType)\n",
    "data.articleType=labencod.transform(data.articleType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "labels=to_categorical(data.articleType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "tr_x,te_x,tr_y,te_y=train_test_split(images,labels,train_size=0.90)\n",
    "image_train=[]\n",
    "image_valid=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in tr_x:\n",
    "    image=decoder_images(img,60,60)\n",
    "    images_train.append(image)\n",
    "\n",
    "for img in te_x:\n",
    "    image=decoder_images(img,60,60)\n",
    "    images_valid.append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import .layers import convolution2d,max_pooling2d,fc_dense,out_dense,batch_normalization\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior() \n",
    "x=tf.placeholder(tf.float32,name='x',shape=[None,60,60,3])\n",
    "y=tf.placeholder(tf.int32,name='y',shape=[None,labels.shape[1]])\n",
    "\n",
    "conv1_1 = convolution2d('conv1_1', x, 64, stride=[1, 1, 1, 1])\n",
    "conv1_2 = convolution2d('conv1_2', conv1_1, 64, stride=[1, 1, 1, 1])\n",
    "pool1 = max_pooling2d(pool1', conv1_2, kernel=[1, 2, 2, 1], stride=[1, 2, 2, 1])\n",
    "\n",
    "conv2_1 = convolution2d('conv2_1', pool1, 128, stride=[1, 1, 1, 1])\n",
    "conv2_2 = convolution2d('conv2_2', conv2_1, 128, stride=[1, 1, 1, 1])\n",
    "pool2 = max_pooling2d('pool2', conv2_2, kernel=[1, 2, 2, 1], stride=[1, 2, 2, 1])\n",
    "\n",
    "conv3_1 = convolution2d('conv3_1', pool2, 256, stride=[1, 1, 1, 1])\n",
    "conv3_2 = convolution2d('conv3_2', conv3_1, 256, stride=[1, 1, 1, 1])\n",
    "conv3_3 = convolution2d('conv3_3', conv3_2, 256, stride=[1, 1, 1, 1])\n",
    "pool3 =max_pooling2d('pool3', conv3_3, kernel=[1, 2, 2, 1], stride=[1, 2, 2, 1])\n",
    "\n",
    "conv4_1 = convolution2d('conv4_1', pool3, 512, stride=[1, 1, 1, 1])\n",
    "conv4_2 = convolution2d('conv4_2', conv4_1, 512, stride=[1, 1, 1, 1])\n",
    "conv4_3 = convolution2d('conv4_3', conv4_2, 512, stride=[1, 1, 1, 1])\n",
    "pool4 = max_pooling2d('pool4', conv4_3, kernel=[1, 2, 2, 1], stride=[1, 2, 2, 1])\n",
    "\n",
    "conv5_1 = convolution2d('conv5_1', pool4, 512, stride=[1, 1, 1, 1])\n",
    "conv5_2 = convolution2d('conv5_2', conv5_1, 512, stride=[1, 1, 1, 1])\n",
    "conv5_3 = convolution2d('conv5_3', conv5_2, 512, stride=[1, 1, 1, 1])\n",
    "pool5 =max_pooling2d('pool5', conv5_3, kernel=[1, 2, 2, 1], stride=[1, 2, 2, 1])\n",
    "\n",
    "fc6 = fc_dense('fc6', pool5, out_nodes=4096)\n",
    "batch_norm1 = batch_normalization('batch_norm1', fc6)\n",
    "fc7 = fc_dense('fc7', batch_norm1, out_nodes=4096)\n",
    "batch_norm2 = batch_normalization('batch_norm2', fc7)\n",
    "logits = out_dense('fc8', batch_norm2, out_nodes=labels.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.utcnow( ).strftime( \"%Y%m%d%H%M%S\" )\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir,now)\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "                logits=logits, labels=y, name='cross-entropy'))\n",
    "optimizer =   tf.train.RMSPropOptimizer(learning_rate=0.00001,momentum=0.6,decay=0.7,epsilon=1e-10).minimize(cross_entropy )\n",
    "\n",
    "\n",
    "mse_summary = tf.summary.scalar( 'cross-entropy' , cross_entropy  )\n",
    "file_writer = tf.summary.FileWriter( logdir , tf.get_default_graph( ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "sacer=tf.train.Saver()\n",
    "numb=0\n",
    "epochs=30\n",
    "for i in range(epochs):\n",
    "    numb+=10\n",
    "    print('epoch',i)\n",
    "    for j in range(len(tr_x)):\n",
    "        x1=images_train[j]\n",
    "        y1=tr_y[j]\n",
    "        y1=np.expand_dims(y1,axis=0)\n",
    " \n",
    "        summ=sess.run(mse_summary,feed_dict={x:x1,y:y1})\n",
    "        file_writer.add_summary(summ, numb)\n",
    "        sess.run(optimizer,feed_dict={x:x1,y:y1})\n",
    "        \n",
    "    loss=sess.run(cross_entropy,feed_dict={x:x1,y:y1})\n",
    "\n",
    "    print('loss:=====================================',loss)\n",
    "  \n",
    "sacer_Path=sacer.save(sess,'path_name.ckpt')       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    metrix_class=[]\n",
    "    sacer.restore(sess,\"path_name.ckpt\")\n",
    "   \n",
    "   \n",
    "    for j in range(len(te_x)):\n",
    "        x1=images_valid[j]\n",
    "    \n",
    "        print('TEST TAG : ',te_y[j].argmax())\n",
    "        a=tf.nn.softmax(logits).eval(feed_dict={x:x1})\n",
    "        pred=np.argmax(a,axis=1)\n",
    "        print('Predictions on test data : ',pred)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
